{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7dYaHH1OPV8"
      },
      "source": [
        "## Dataset Link: https://github.com/Aniruddha-Tapas/Predicting-Diseases-From-Symptoms/tree/master/Manual-Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV-ZSW1vemms"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-ciV-gAJgRR",
        "outputId": "1a34da42-ef43-4d67-9d6d-2e60535b9f9f"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade tensorflow_federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAI_vuasKl8C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxacZUFwLwnx"
      },
      "source": [
        "## Directory-Paths\n",
        "\n",
        "**No need to change if dataset or path is not changed!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql5GbqhZJicS"
      },
      "outputs": [],
      "source": [
        "# base directory\n",
        "BASE_DIR = '/content/drive/MyDrive/Research Works/International Papers/BigComp/2020/disease_prediction_attention'\n",
        "\n",
        "# data directory\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'data_attention_federated')\n",
        "\n",
        "# manual input directory\n",
        "MANUAL_INPUT_DIR = os.path.join(DATA_DIR, 'manual_input')\n",
        "\n",
        "# output directory\n",
        "OUTPUT_DIR = os.path.join(DATA_DIR, 'output_2021July')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# current output directory\n",
        "CURRENT_OUTPUT_DIR = os.path.join(OUTPUT_DIR, current_time)\n",
        "os.makedirs(CURRENT_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# output log directory\n",
        "LOG_DIR = os.path.join(CURRENT_OUTPUT_DIR, 'logs')\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "# output image directory\n",
        "IMG_DIR = os.path.join(CURRENT_OUTPUT_DIR, 'image')\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "# output model directory\n",
        "MODEL_DIR = os.path.join(CURRENT_OUTPUT_DIR, 'model')\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXFIJ70mT6Tt"
      },
      "source": [
        "## Analysis on Manual Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNJERF4LMLq3",
        "outputId": "c6b6f26a-2de7-4cc2-852f-bf7fdba8731b"
      },
      "outputs": [],
      "source": [
        "for dirname, _, filenames in os.walk(MANUAL_INPUT_DIR):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaE_HxNMMwgR"
      },
      "source": [
        "### Read CSV to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "g5Y6-rd4Mgty",
        "outputId": "ffb5f561-3e8c-4d2e-83d3-fb1e5f176728"
      },
      "outputs": [],
      "source": [
        "# Training data\n",
        "train_df = pd.read_csv(os.path.join(MANUAL_INPUT_DIR, 'Training.csv'))\n",
        "print(\"Dataset with rows {} and columns {}\".format(train_df.shape[0],train_df.shape[1]))\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "bY_ykFaGM3JZ",
        "outputId": "1a257a56-1b65-4413-d303-0e576a9323ca"
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "B-3NG51jMnii",
        "outputId": "704c885a-06c8-49a0-ea9f-8d89e061d73f"
      },
      "outputs": [],
      "source": [
        "# Testing data\n",
        "test_df = pd.read_csv(os.path.join(MANUAL_INPUT_DIR, 'Testing.csv'))\n",
        "print(\"Dataset with rows {} and columns {}\".format(test_df.shape[0],test_df.shape[1]))\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2tOdPvhxM9N9",
        "outputId": "7e2b98f4-254a-4707-8d35-d2e1328efd40"
      },
      "outputs": [],
      "source": [
        "test_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK52cnlvewKN"
      },
      "source": [
        "## Data Preprocessing (manual data already split into 5 clients and datafiles created)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtyPMgROxtFg"
      },
      "outputs": [],
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20APoNC6mPhu"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"_\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿_]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oSdtmF3Vkga"
      },
      "outputs": [],
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = []\n",
        "  symptoms_list = []\n",
        "  diseases_list = []\n",
        "\n",
        "  f = open(path, 'r')\n",
        "  rd = csv.reader(f, delimiter=' ', skipinitialspace=True)\n",
        "\n",
        "  for row in rd:\n",
        "    lines.append(row)\n",
        "\n",
        "  for i in range(len(lines)):\n",
        "    # symptom = '<start> ' + str(lines[i][:-1]) + ' <end>'\n",
        "    symptom = '<start> '\n",
        "    for symp_item in lines[i][:-1]:\n",
        "      symptom += symp_item + ' '\n",
        "    symptom += '<end>'\n",
        "    symptoms_list.append(symptom)\n",
        "\n",
        "    disease_arr = lines[i][-1].split('_')\n",
        "    disease = '<start> '\n",
        "    for disease_item in disease_arr:\n",
        "      disease += disease_item + ' '\n",
        "    disease += '<end>'\n",
        "    diseases_list.append(disease)\n",
        "\n",
        "  return symptoms_list, diseases_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w-uAP4XyA7A"
      },
      "outputs": [],
      "source": [
        "# train_data = pd.read_csv(os.path.join(MANUAL_INPUT_DIR, 'Training.csv'))\n",
        "# train_data.prognosis = train_data.prognosis.apply(lambda x: x.replace(' ','_'))\n",
        "\n",
        "# # shuffle dataframe\n",
        "# shuffled_train_data = shuffle(train_data)\n",
        "\n",
        "# num_clients = 5\n",
        "# split_interval = 1000\n",
        "# client_data = []\n",
        "# processed_client = []\n",
        "\n",
        "# for i in range(num_clients):\n",
        "#     data = shuffled_train_data.iloc[split_interval*i:split_interval*(i+1)]\n",
        "#     client_data.append(data)\n",
        "#     client_data[i].to_csv(os.path.join(MANUAL_INPUT_DIR, 'client_'+str(i)+'.csv'), index=False)\n",
        "\n",
        "#     client = pd.DataFrame()\n",
        "#     for j in range(len(client_data[i].columns)):\n",
        "#       z = client_data[i].iloc[:,j].replace(1, client_data[i].columns[j])\n",
        "#       client = client.append(z)\n",
        "#     client = client.replace(0, '')\n",
        "#     client = client.T\n",
        "    \n",
        "#     processed_client.append(client)\n",
        "#     np.savetxt(os.path.join(MANUAL_INPUT_DIR, 'data_file_' + str(i) + '.txt'), processed_client[i].values, fmt='%s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz5Miv31LHTZ"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0H2T4BnLogB"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  symptoms_list, diseases_list = create_dataset(path, num_examples)\n",
        "\n",
        "  in_tensor, in_token = tokenize(symptoms_list)\n",
        "  tar_tensor, tar_token = tokenize(diseases_list)\n",
        "\n",
        "  return in_tensor, tar_tensor, in_token, tar_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xyAbPj4LpwB"
      },
      "outputs": [],
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(\"%d ----> %s\" % (t, lang.index_word[t]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5EYYWcJpHhu"
      },
      "source": [
        "### Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHk1AGwJLuHr",
        "outputId": "317cb6d8-2913-4b01-9cdd-97e214ac9a61"
      },
      "outputs": [],
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_clients = 5\n",
        "num_examples = 1000\n",
        "\n",
        "symptom_tensor = []\n",
        "disease_tensor = []\n",
        "symptom_token = []\n",
        "disease_token = []\n",
        "\n",
        "max_length_symptom = []\n",
        "max_length_disease = []\n",
        "\n",
        "symptom_tensor_train = []\n",
        "symptom_tensor_val = []\n",
        "disease_tensor_train = []\n",
        "disease_tensor_val = []\n",
        "\n",
        "for i in range(num_clients):\n",
        "  in_tensor, tar_tensor, in_token, tar_token = load_dataset(os.path.join(MANUAL_INPUT_DIR, 'data_file_' + str(i) + '.txt'), num_examples)\n",
        "  symptom_tensor.append(in_tensor)\n",
        "  disease_tensor.append(tar_tensor)\n",
        "  symptom_token.append(in_token)\n",
        "  disease_token.append(tar_token)\n",
        "\n",
        "  # Calculate max_length of the tensors\n",
        "  symptom_len, disease_len = symptom_tensor[i].shape[1], disease_tensor[i].shape[1]\n",
        "  max_length_symptom.append(symptom_len)\n",
        "  max_length_disease.append(disease_len)\n",
        "  \n",
        "  # Creating training and validation sets using an 80-20 split\n",
        "  in_tensor_train, in_tensor_val, tar_tensor_train, tar_tensor_val = train_test_split(symptom_tensor[i], disease_tensor[i], test_size=0.2)\n",
        "\n",
        "  symptom_tensor_train.append(in_tensor_train)\n",
        "  symptom_tensor_val.append(in_tensor_val)\n",
        "  \n",
        "  disease_tensor_train.append(tar_tensor_train)\n",
        "  disease_tensor_val.append(tar_tensor_val)\n",
        "\n",
        "  # Show length\n",
        "  print(len(symptom_tensor_train[i]), len(disease_tensor_train[i]), len(symptom_tensor_val[i]), len(disease_tensor_val[i]))\n",
        "\n",
        "  # print('Symptom_' + str(i) + '; index to word mapping')\n",
        "  # convert(symptom_token[i], symptom_tensor_train[i][0])\n",
        "  # print()\n",
        "  # print('Disease_' + str(i) + '; index to word mapping')\n",
        "  # convert(disease_token[i], disease_tensor_train[i][0])\n",
        "  # print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8DSV0cGpC5S"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ufpBHSWBpCRy",
        "outputId": "182180c9-aa5a-41fb-ffc1-22e7f2d1eaff"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(os.path.join(MANUAL_INPUT_DIR, 'Testing.csv'))\n",
        "test_data.prognosis = test_data.prognosis.apply(lambda x: x.replace(' ','_'))\n",
        "\n",
        "test_processed_df = pd.DataFrame()\n",
        "for i in range(len(test_data.columns)):\n",
        "  z = test_data.iloc[:,i].replace(1, test_data.columns[i])\n",
        "  test_processed_df = test_processed_df.append(z)\n",
        "test_processed_df = test_processed_df.replace(0, '')\n",
        "test_processed_df = test_processed_df.T\n",
        "test_processed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY2Ybg8RrDQs"
      },
      "outputs": [],
      "source": [
        "np.savetxt(os.path.join(MANUAL_INPUT_DIR, 'test_file.txt'), test_processed_df.values, fmt='%s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djkm5VcyrEuO"
      },
      "outputs": [],
      "source": [
        "def create_testset(path, num_examples):\n",
        "  lines = []\n",
        "  symptoms_list = []\n",
        "  diseases_list = []\n",
        "\n",
        "  f = open(path, 'r')\n",
        "  rd = csv.reader(f, delimiter=' ', skipinitialspace=True)\n",
        "  for row in rd:\n",
        "    lines.append(row)\n",
        "  for i in range(len(lines)):\n",
        "    symptom = ''\n",
        "    for symp_item in lines[i][:-1]:\n",
        "      symptom += symp_item + ' '\n",
        "    symptoms_list.append(symptom)\n",
        "    \n",
        "    disease_arr = lines[i][-1].split('_')\n",
        "    disease = '<start> '\n",
        "    for disease_item in disease_arr:\n",
        "      disease += disease_item + ' '\n",
        "    disease += '<end>'\n",
        "    diseases_list.append(disease)\n",
        "\n",
        "  return symptoms_list, diseases_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P67JEa2gKY_"
      },
      "source": [
        "## Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgYj21fRdyMD",
        "outputId": "c8c2d972-b27d-4b2d-d34a-cf6493dc5eeb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "train_buffer_size = []\n",
        "train_step_per_epoch = []\n",
        "train_vocab_inp_size = []\n",
        "train_vocab_tar_size = []\n",
        "train_dataset = []\n",
        "\n",
        "val_buffer_size = []\n",
        "val_step_per_epoch = []\n",
        "val_vocab_inp_size = []\n",
        "val_vocab_tar_size = []\n",
        "val_dataset = []\n",
        "\n",
        "train_example_input_batch = []\n",
        "train_example_target_batch = []\n",
        "\n",
        "val_example_input_batch = []\n",
        "val_example_target_batch = []\n",
        "\n",
        "for i in range(num_clients):\n",
        "  train_buff_sz = len(symptom_tensor_train[i])\n",
        "  train_buffer_size.append(train_buff_sz)\n",
        "\n",
        "  val_buff_sz = len(symptom_tensor_val[i])\n",
        "  val_buffer_size.append(val_buff_sz)\n",
        "\n",
        "  train_epoch_step = len(symptom_tensor_train[i])//BATCH_SIZE\n",
        "  train_step_per_epoch.append(train_epoch_step)\n",
        "\n",
        "  val_epoch_step = len(symptom_tensor_val[i])//BATCH_SIZE\n",
        "  val_step_per_epoch.append(val_epoch_step)\n",
        "\n",
        "  train_vocab_in_sz = len(symptom_token[i].word_index)+1\n",
        "  train_vocab_inp_size.append(train_vocab_in_sz)\n",
        "  train_vocab_tar_sz = len(disease_token[i].word_index)+1\n",
        "  train_vocab_tar_size.append(train_vocab_tar_sz)\n",
        "\n",
        "  val_vocab_in_sz = len(symptom_token[i].word_index)+1\n",
        "  val_vocab_inp_size.append(val_vocab_in_sz)\n",
        "  val_vocab_tar_sz = len(disease_token[i].word_index)+1\n",
        "  val_vocab_tar_size.append(val_vocab_tar_sz)\n",
        "\n",
        "  train_dtst = tf.data.Dataset.from_tensor_slices((symptom_tensor_train[i], disease_tensor_train[i])).shuffle(train_buffer_size[i])\n",
        "  train_dtst = train_dtst.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  train_dataset.append(train_dtst)\n",
        "\n",
        "  val_dtst = tf.data.Dataset.from_tensor_slices((symptom_tensor_val[i], disease_tensor_val[i])).shuffle(val_buffer_size[i])\n",
        "  val_dtst = val_dtst.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  val_dataset.append(val_dtst)\n",
        "\n",
        "  train_eg_in_batch, train_eg_tar_batch = next(iter(train_dataset[i]))\n",
        "  train_example_input_batch.append(train_eg_in_batch)\n",
        "  train_example_target_batch.append(train_eg_tar_batch)\n",
        "  print(\"Train --> \", train_example_input_batch[i].shape, train_example_target_batch[i].shape)\n",
        "\n",
        "  val_eg_in_batch, val_eg_tar_batch = next(iter(val_dataset[i]))\n",
        "  val_example_input_batch.append(val_eg_in_batch)\n",
        "  val_example_target_batch.append(val_eg_tar_batch)\n",
        "  print(\"Val --> \", val_example_input_batch[i].shape, val_example_target_batch[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J-mdkPTq2yv"
      },
      "source": [
        "## Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aB5V2_6gNmk"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnerqKq3k6nG"
      },
      "source": [
        "### Local Encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh5ni4z31xiC",
        "outputId": "3d95a99c-3265-4a97-e197-19b695b8993d"
      },
      "outputs": [],
      "source": [
        "local_encoder = []\n",
        "local_hidden = []\n",
        "local_output = []\n",
        "\n",
        "for i in range(num_clients):\n",
        "  local_enc = Encoder(train_vocab_inp_size[i], embedding_dim, units, BATCH_SIZE)\n",
        "  local_encoder.append(local_enc)\n",
        "\n",
        "  loc_hid = local_encoder[i].initialize_hidden_state()\n",
        "  local_hidden.append(loc_hid)\n",
        "\n",
        "  loc_out, loc_hid = local_encoder[i](train_example_input_batch[i], loc_hid)\n",
        "  local_output.append(loc_out)\n",
        "\n",
        "  print ('Encoder ' + str(i) + ' output shape: (batch size, sequence length, units) {}'.format(local_output[i].shape))\n",
        "  print ('Encoder ' + str(i) + ' hidden state shape: (batch size, units) {}'.format(local_hidden[i].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htCWIC8Vk8Z_"
      },
      "source": [
        "### Global Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A-dWecX1hmI",
        "outputId": "cb6d4069-7bea-4657-ba50-d6613017232f"
      },
      "outputs": [],
      "source": [
        "global_encoder = Encoder(train_vocab_inp_size[0], embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "global_hidden = global_encoder.initialize_hidden_state()\n",
        "\n",
        "global_output, global_hidden = global_encoder(train_example_input_batch[0], global_hidden)\n",
        "\n",
        "print ('Global Encoder output shape: (batch size, sequence length, units) {}'.format(global_output.shape))\n",
        "print ('Global Encoder  Hidden state shape: (batch size, units) {}'.format(global_hidden.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjehsCzc4-ka"
      },
      "source": [
        "## Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv5oyvVL4-UG"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKYTDkRlFRn"
      },
      "source": [
        "### Local Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVBy_XLBkumJ",
        "outputId": "8f361cc4-05d8-4b80-9c28-a36d063f3a52"
      },
      "outputs": [],
      "source": [
        "attention_layer = []\n",
        "attention_result = []\n",
        "attention_weights = []\n",
        "\n",
        "for i in range(num_clients):\n",
        "  att_layer = BahdanauAttention(10)\n",
        "  att_result, att_weights = att_layer(local_hidden[i], local_output[i])\n",
        "\n",
        "  attention_layer.append(att_layer)\n",
        "  attention_result.append(att_result)\n",
        "  attention_weights.append(att_weights)\n",
        "\n",
        "  print('Attention ' + str(i) + ' result shape: (batch size, units) {}'.format(attention_result[i].shape))\n",
        "  print('Attention ' + str(i) + ' weights shape: (batch_size, sequence_length, 1) {}'.format(attention_weights[i].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E6j5Mv1lIC-"
      },
      "source": [
        "### Global Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G7UWzPEk4eX",
        "outputId": "4a28c24b-1836-4633-9e22-48926aba02f6"
      },
      "outputs": [],
      "source": [
        "global_attention = BahdanauAttention(10)\n",
        "global_attention_result, global_attention_weight = global_attention(global_hidden, global_output)\n",
        "\n",
        "print('Global Attention result shape: (batch size, units) {}'.format(global_attention_result.shape))\n",
        "print('Global Attention weights shape: (batch_size, sequence_length, 1) {}'.format(global_attention_weight.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbKDO0d6lpAj"
      },
      "source": [
        "## Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EYxZzNill7v"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhYyEcIMnEYP"
      },
      "source": [
        "### Local Decoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDGAGE_ynBJG",
        "outputId": "b15779eb-14ad-48c3-d1e3-c0f6f744bae9"
      },
      "outputs": [],
      "source": [
        "local_decoder = []\n",
        "\n",
        "local_decoder_output = []\n",
        "\n",
        "for i in range(num_clients):\n",
        "  local_dec = Decoder(train_vocab_tar_size[i], embedding_dim, units, BATCH_SIZE)\n",
        "  local_decoder.append(local_dec)\n",
        "\n",
        "  loc_dec_out, _, _ = local_decoder[i](tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      local_hidden[i], local_output[i])\n",
        "  local_decoder_output.append(loc_dec_out)\n",
        "  print ('Decoder ' + str(i) + ' output shape: (batch_size, vocab size) {}'.format(local_decoder_output[i].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfNCTw5PnwnX"
      },
      "source": [
        "### Global Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7XfQwMynhHg",
        "outputId": "13927636-6cdb-470d-b548-d04c243a4a91"
      },
      "outputs": [],
      "source": [
        "global_decoder = Decoder(train_vocab_tar_size[0], embedding_dim, units, BATCH_SIZE)\n",
        "global_decoder_output, _, _ = global_decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      global_hidden, global_output)\n",
        "print ('Global Decoder output shape: (batch_size, vocab size) {}'.format(global_decoder_output.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MntcaNi1n2MH"
      },
      "source": [
        "## Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoV7d9s_ntb_"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoW3xgTcyILC"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VQV5EEGoTrn"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = []\n",
        "checkpoint_prefix = []\n",
        "checkpoint = []\n",
        "\n",
        "for i in range(num_clients):\n",
        "  ckpt_dir = os.path.join(CURRENT_OUTPUT_DIR, 'training_checkpoints_' + str(i))\n",
        "  checkpoint_dir.append(ckpt_dir)\n",
        "  ckpt_prefix = os.path.join(checkpoint_dir[i], \"ckpt\")\n",
        "  checkpoint_prefix.append(ckpt_prefix)\n",
        "  ckpt = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=local_encoder[i],\n",
        "                                 decoder=local_decoder[i])\n",
        "  checkpoint.append(ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0kGn_HFpUDZ"
      },
      "source": [
        "## Training Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2fucG9dpVw_"
      },
      "outputs": [],
      "source": [
        "def train_step(inp, targ, enc_hidden, encoder, decoder, target_lang):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "      dec_hidden = enc_hidden\n",
        "      dec_input = tf.expand_dims([target_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "      # Teacher forcing - feeding the target as the next input\n",
        "      for t in range(1, targ.shape[1]):\n",
        "        # passing enc_output to the decoder\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "        # using teacher forcing\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6E1gIPupnvW"
      },
      "outputs": [],
      "source": [
        "def get_model_loss(inp, targ, enc_hidden, encoder, decoder, target_lang):\n",
        "    \n",
        "    loss = 0\n",
        "\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6ckFhjgpoRQ",
        "outputId": "24622b6e-7865-4952-95a2-68b6e757f951"
      },
      "outputs": [],
      "source": [
        "enc_hidden = []\n",
        "\n",
        "for i in range(num_clients):\n",
        "    enc_hid = local_encoder[i].initialize_hidden_state()\n",
        "    enc_hidden.append(enc_hid)\n",
        "\n",
        "    total_val_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(val_dataset[i].take(val_step_per_epoch[i])):\n",
        "\n",
        "        batch_loss = get_model_loss(inp, targ, enc_hidden[i], local_encoder[i], local_decoder[i], disease_token[i] )\n",
        "        total_val_loss += batch_loss\n",
        "\n",
        "    print('Client ' + str(i) + ' val_loss: {:.4f}'.format(total_val_loss / val_step_per_epoch[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp3gOv6444jv"
      },
      "outputs": [],
      "source": [
        "loss_history = {}\n",
        "for client in range(num_clients):\n",
        "    loss_history[client] = []\n",
        "\n",
        "global_index = 5\n",
        "loss_history[global_index] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhS793yj5Q_R",
        "outputId": "3e3e356d-2e8d-49c2-8494-4b62c6d03bbb"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5\n",
        "NUM_ROUNDS = 30\n",
        "\n",
        "for round in range(NUM_ROUNDS):\n",
        "\n",
        "    print('Round: ', str(round))\n",
        "    enc_hidden = []\n",
        "    for i in range(num_clients):\n",
        "        local_encoder[i].set_weights(global_encoder.get_weights())\n",
        "        local_decoder[i].set_weights(global_decoder.get_weights())\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            start = time.time()\n",
        "\n",
        "            enc_hid = local_encoder[i].initialize_hidden_state()\n",
        "            enc_hidden.append(enc_hid)\n",
        "\n",
        "            total_loss = 0\n",
        "\n",
        "            for (batch, (inp, targ)) in enumerate(train_dataset[i].take(train_step_per_epoch[i])):\n",
        "                batch_loss = train_step(inp, targ, enc_hidden[i], local_encoder[i], local_decoder[i], disease_token[i])\n",
        "                total_loss += batch_loss\n",
        "\n",
        "\n",
        "            # saving (checkpoint) the model every 2 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                checkpoint[i].save(file_prefix = checkpoint_prefix[i])\n",
        "\n",
        "    #   print('Client ' + str(i) + ' Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch[i]))\n",
        "\n",
        "        train_batch_loss = 0\n",
        "        for (batch, (inp, targ)) in enumerate(train_dataset[i].take(train_step_per_epoch[i])):\n",
        "            batch_loss = train_step(inp, targ, enc_hidden[i], local_encoder[i], local_decoder[i], disease_token[i])\n",
        "            train_batch_loss += batch_loss\n",
        "\n",
        "        train_loss = train_batch_loss / train_step_per_epoch[i]\n",
        "\n",
        "        # Calculate validation loss on all valid data\n",
        "        for dtst_index in range(num_clients):\n",
        "\n",
        "            val_batch_loss = 0\n",
        "            val_loss_dtst_list = []\n",
        "\n",
        "            for (batch, (inp, targ)) in enumerate(val_dataset[dtst_index].take(val_step_per_epoch[dtst_index])):\n",
        "\n",
        "                batch_loss = get_model_loss(inp, targ, enc_hidden[i], local_encoder[i], local_decoder[i], disease_token[dtst_index])\n",
        "                val_batch_loss += batch_loss\n",
        "\n",
        "            val_loss_dtst = val_batch_loss / val_step_per_epoch[dtst_index]\n",
        "            val_loss_dtst_list.append(val_loss_dtst)\n",
        "\n",
        "\n",
        "        val_loss = sum(val_loss_dtst_list) / num_clients\n",
        "        print()\n",
        "        print('Client: ', i)\n",
        "        print('train_loss: ', train_loss)\n",
        "        print('val_loss: ', val_loss)\n",
        "       \n",
        "\n",
        "        loss_history[i].append((train_loss, val_loss))\n",
        "\n",
        "    # train_batch_loss = 0\n",
        "\n",
        "    #############################################################################################################\n",
        "\n",
        "\n",
        "    # Get encoder weights\n",
        "    enc_wgt = []\n",
        "    encoder_weights = []\n",
        "    encoder_total_weight = 0\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        enc_w = local_encoder[i].get_weights()\n",
        "        enc_wgt.append(enc_w)\n",
        "\n",
        "        enc_weight = np.array(enc_wgt[i])\n",
        "        encoder_weights.append(enc_weight)\n",
        "\n",
        "        encoder_total_weight += encoder_weights[i]\n",
        "        encoder_avg_weight = encoder_total_weight / num_clients\n",
        "\n",
        "    # print(encoder_avg_weight)\n",
        "\n",
        "    global_encoder.set_weights(encoder_avg_weight)\n",
        "\n",
        "    # Get decoder weights\n",
        "    dec_wgt = []\n",
        "    decoder_weights = []\n",
        "    decoder_total_weight = 0\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        dec_w = local_decoder[i].get_weights()\n",
        "        dec_wgt.append(dec_w)\n",
        "\n",
        "        dec_weight = np.array(dec_wgt[i])\n",
        "        decoder_weights.append(dec_weight)\n",
        "\n",
        "        decoder_total_weight += decoder_weights[i]\n",
        "        decoder_avg_weight = decoder_total_weight / num_clients\n",
        "\n",
        "    # print(decoder_avg_weight)\n",
        "\n",
        "    global_decoder.set_weights(decoder_avg_weight)\n",
        "\n",
        "    #############################################################################################################\n",
        "    # Calculate loss for global model\n",
        "\n",
        "    enc_hid = global_encoder.initialize_hidden_state()\n",
        "\n",
        "    for dtst_index in range(num_clients):\n",
        "\n",
        "        # Calculate train loss on all valid data\n",
        "        train_batch_loss = 0\n",
        "        train_loss_dtst_list = []\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(train_dataset[dtst_index].take(train_step_per_epoch[dtst_index])):\n",
        "\n",
        "            batch_loss = get_model_loss(inp, targ, enc_hid, global_encoder, global_decoder, disease_token[dtst_index])\n",
        "            train_batch_loss += batch_loss\n",
        "\n",
        "        train_loss_dtst = train_batch_loss / train_step_per_epoch[dtst_index]\n",
        "        train_loss_dtst_list.append(train_loss_dtst)\n",
        "\n",
        "        # Calculate validation loss on all valid data\n",
        "        val_batch_loss = 0\n",
        "        val_loss_dtst_list = []\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(val_dataset[dtst_index].take(val_step_per_epoch[dtst_index])):\n",
        "\n",
        "            batch_loss = get_model_loss(inp, targ, enc_hid, global_encoder, global_decoder, disease_token[dtst_index])\n",
        "            val_batch_loss += batch_loss\n",
        "\n",
        "        val_loss_dtst = val_batch_loss / val_step_per_epoch[dtst_index]\n",
        "        val_loss_dtst_list.append(val_loss_dtst)\n",
        "\n",
        "\n",
        "    val_loss = sum(val_loss_dtst_list) / num_clients\n",
        "    train_loss = sum(train_loss_dtst_list) / num_clients\n",
        "    print()\n",
        "    print('Global Model')\n",
        "    print('train_loss: ', train_loss)\n",
        "    print('val_loss: ', val_loss)\n",
        "    loss_history[global_index].append((train_loss, val_loss))\n",
        "    print('#########################################################')\n",
        "\n",
        "    #############################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pqI3zL36Leh"
      },
      "outputs": [],
      "source": [
        "loss_history_file = os.path.join(CURRENT_OUTPUT_DIR, 'training_history.txt')\n",
        "\n",
        "with open(loss_history_file, 'wb') as file_pi:\n",
        "\n",
        "    pickle.dump(loss_history, file_pi)\n",
        "\n",
        "# training_history = pickle.load(open(training_history_file, \"rb\"))\n",
        "# train_mse, test_mse, train_mae, test_mae = training_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGvIYQiq6L8b"
      },
      "outputs": [],
      "source": [
        "loss_history = pickle.load(open(loss_history_file, \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwWR0m5T2497"
      },
      "outputs": [],
      "source": [
        "def plot_graph(model_loss_history, model_type='Client ', index=''):\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "\n",
        "  for item in model_loss_history:\n",
        "      train_loss.append(item[0].numpy())\n",
        "      val_loss.append(item[1].numpy())\n",
        "\n",
        "  plt.figure(figsize=(10,4))\n",
        "  plt.plot(train_loss, label='train')\n",
        "  plt.plot(val_loss, label='val')\n",
        "  plt.title('Training History ({}{}) '.format(model_type, index))\n",
        "  plt.xlabel('Rounds')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.grid(True)\n",
        "  plt.legend()\n",
        "  img_path = os.path.join(IMG_DIR, '{}{}_history.png'.format(model_type, index))\n",
        "  plt.savefig(img_path, dpi=300, bbox_inches = 'tight')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Htj7L6Td3c2p",
        "outputId": "db05b52b-a646-476e-cb72-8c0932a76c7c"
      },
      "outputs": [],
      "source": [
        "for i in range(num_clients):\n",
        "  plot_graph(loss_history[i], index=i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "RgyFIQWp3i-s",
        "outputId": "c96a6cd2-5d9a-487c-cfe8-7ee9f03ddf7e"
      },
      "outputs": [],
      "source": [
        "global_index = 5\n",
        "plot_graph(loss_history[global_index], model_type='Global')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "bizI-65k3mzr",
        "outputId": "b559af03-49de-4e9b-9bef-91a54b586139"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "for i in range(6):    \n",
        "    \n",
        "    train_loss = []\n",
        "\n",
        "    for item in loss_history[i]:\n",
        "        train_loss.append(item[0].numpy())\n",
        "    plt.plot(train_loss, label=('Client {}'.format(i+1) if i<5 else 'Global Model'))\n",
        "\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "img_path = os.path.join(IMG_DIR, 'training_loss.png')\n",
        "plt.savefig(img_path, dpi=300, bbox_inches = 'tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "YZW3nYSC3tq_",
        "outputId": "55c1f597-5b2e-4d15-8182-1aa7759d867a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "for i in range(6):    \n",
        "    \n",
        "    valid_loss = []\n",
        "\n",
        "    for item in loss_history[i]:\n",
        "        valid_loss.append(item[1].numpy())\n",
        "    plt.plot(valid_loss, label=('Client {}'.format(i+1) if i<5 else 'Global Model'))\n",
        "\n",
        "plt.title('Validation Loss')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "img_path = os.path.join(IMG_DIR, 'validation_loss.png')\n",
        "plt.savefig(img_path, dpi=300, bbox_inches = 'tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-J5-hS26N4R"
      },
      "outputs": [],
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence, img_path):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.savefig(img_path, dpi=300, bbox_inches = 'tight')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqSFBS3K6PoR"
      },
      "outputs": [],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "for i in range(num_clients):\n",
        "  checkpoint[i].restore(tf.train.latest_checkpoint(checkpoint_dir[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6womKeli6RbK"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence, max_len_tar, max_len_in, in_lan, encoder, tar_lan, decoder):\n",
        "  attention_plot = np.zeros((max_len_tar, max_len_in))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [in_lan.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_len_in,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([tar_lan.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_len_tar):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += tar_lan.index_word[predicted_id] + ' '\n",
        "\n",
        "    if tar_lan.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzxCZ9_a6TYZ"
      },
      "outputs": [],
      "source": [
        "def translate(sentence, max_len_tar, max_len_in, in_lan, encoder, tar_lan, decoder, img_path):\n",
        "  result, sentence, attention_plot = evaluate(sentence, max_len_tar, max_len_in, in_lan, encoder, tar_lan, decoder)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '), img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "GWI5lfSW0Z2E",
        "outputId": "a98ebef4-78c7-4332-d53f-9ef466021680"
      },
      "outputs": [],
      "source": [
        "img_path = os.path.join(IMG_DIR, 'sample_1.png')\n",
        "translate(u'itching skin_rash nodal_skin_eruptions continuous_sneezing chest_pain', max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "imOeF4SY6VAq",
        "outputId": "fb851974-0613-4e93-96d6-23e31103b56c"
      },
      "outputs": [],
      "source": [
        "img_path = os.path.join(IMG_DIR, 'sample_2.png')\n",
        "translate(u'continuous_sneezing chills fatigue cough high_fever headache muscle_pain chest_pain swelled_lymph_nodes malaise phlegm throat_irritation redness_of_eyes sinus_pressure runny_nose congestion loss_of_smell', max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5rBV_1_pNCs"
      },
      "outputs": [],
      "source": [
        "sym, dis = create_testset(os.path.join(MANUAL_INPUT_DIR, 'test_file.txt'), None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "QXErQ20WrsR-",
        "outputId": "72c928b5-4bfc-45d3-a014-143bfd3a7635"
      },
      "outputs": [],
      "source": [
        "print(sym[19])\n",
        "print(dis[19])\n",
        "\n",
        "img_path = os.path.join(IMG_DIR, 'sample_3.png')\n",
        "translate(sym[19], max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "rQ1D5v1ysCJv",
        "outputId": "3a9f6268-1729-4a28-e385-6aa087219be8"
      },
      "outputs": [],
      "source": [
        "print(sym[7])\n",
        "print(dis[7])\n",
        "\n",
        "img_path = os.path.join(IMG_DIR, 'sample_4.png')\n",
        "translate(sym[7], max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "DGFNH3Vmspo_",
        "outputId": "b2dfde00-62f6-4d3f-e2d4-81e5556d8e5c"
      },
      "outputs": [],
      "source": [
        "print(sym[9])\n",
        "print(dis[9])\n",
        "\n",
        "img_path = os.path.join(IMG_DIR, 'sample_5.png')\n",
        "translate(sym[9], max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Axuvu-HHs0Mn",
        "outputId": "134b486c-5d5a-4c2a-bc29-4aee7be85f0d"
      },
      "outputs": [],
      "source": [
        "print(sym[17])\n",
        "print(dis[17])\n",
        "\n",
        "img_path = os.path.join(IMG_DIR, 'sample_6.png')\n",
        "translate(sym[17], max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "p6JH0sqQs_jm",
        "outputId": "ca77f476-a06c-4730-9286-c86c39ba45e1"
      },
      "outputs": [],
      "source": [
        "print(sym[26])\n",
        "print(dis[26])\n",
        "\n",
        "img_path = os.path.join(IMG_DIR, 'sample_7.png')\n",
        "translate(sym[26], max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "uX4C0cDK1MYw",
        "outputId": "594b5430-4d9b-482c-e528-0efd3438925b"
      },
      "outputs": [],
      "source": [
        "print(sym[27])\n",
        "print(dis[27])\n",
        "\n",
        "img_path = os.path.join(IMG_DIR, 'sample_8.png')\n",
        "translate(sym[27], max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "VeO_GMbp1UAj",
        "outputId": "89acb0e4-c7f5-4683-f6e8-fbfca33d2544"
      },
      "outputs": [],
      "source": [
        "print(sym[37])\n",
        "print(dis[37])\n",
        "\n",
        "img_path = os.path.join(IMG_DIR, 'sample_9.png')\n",
        "translate(sym[37], max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "SANxJ4x21abJ",
        "outputId": "158caee8-ad2c-4aa8-e50f-f8cbda2c3ffa"
      },
      "outputs": [],
      "source": [
        "print(sym[1])\n",
        "print(dis[1])\n",
        "\n",
        "img_path = os.path.join(IMG_DIR, 'sample_10.png')\n",
        "translate(sym[1], max_length_disease[0], max_length_symptom[0], symptom_token[0], global_encoder, disease_token[0], global_decoder, img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67EQyJha1ev8",
        "outputId": "387eaec8-fd48-4ade-edc8-081d6eac5e49"
      },
      "outputs": [],
      "source": [
        "print(dis)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
